{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference-Guided Image Captioning Alignment - Exploration Notebook\n",
    "\n",
    "This notebook demonstrates the key capabilities of our preference-guided image captioning system, including model architecture exploration, training pipeline analysis, and evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path().parent / \"src\"))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preference_guided_image_captioning_alignment.utils.config import Config\n",
    "from preference_guided_image_captioning_alignment.models.model import PreferenceGuidedCaptioningModel\n",
    "from preference_guided_image_captioning_alignment.data.preprocessing import ImageProcessor, TextProcessor\n",
    "\n",
    "# Load configuration\n",
    "config = Config(\"../configs/default.yaml\")\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Model config: {config.get_model_config()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with smaller dimensions for exploration\n",
    "model = PreferenceGuidedCaptioningModel(\n",
    "    vision_model=\"openai/clip-vit-base-patch32\",\n",
    "    text_model=\"microsoft/DialoGPT-medium\",\n",
    "    projection_dim=256,  # Smaller for demo\n",
    "    temperature=0.07,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data processors\n",
    "image_processor = ImageProcessor(image_size=224, augment=False)\n",
    "text_processor = TextProcessor(\n",
    "    model_name=\"microsoft/DialoGPT-medium\",\n",
    "    max_length=128,\n",
    ")\n",
    "\n",
    "print(f\"Image processor: {image_processor.image_size}x{image_processor.image_size}\")\n",
    "print(f\"Text processor vocab size: {text_processor.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "sample_image = Image.new('RGB', (224, 224), color='red')\n",
    "sample_captions = [\n",
    "    \"A beautiful red sports car on a winding mountain road\",\n",
    "    \"A magnificent sunset over the ocean with golden reflections\",\n",
    "    \"A cute kitten playing with a ball of yarn\",\n",
    "    \"A modern city skyline illuminated at night\"\n",
    "]\n",
    "\n",
    "# Process image\n",
    "processed_image = image_processor.process_image(sample_image)\n",
    "print(f\"Processed image shape: {processed_image.shape}\")\n",
    "print(f\"Image tensor stats: min={processed_image.min():.3f}, max={processed_image.max():.3f}\")\n",
    "\n",
    "# Process captions\n",
    "processed_captions = text_processor.encode_batch(sample_captions)\n",
    "print(f\"Processed captions shape: {processed_captions['input_ids'].shape}\")\n",
    "print(f\"Caption lengths: {processed_captions['attention_mask'].sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model components\n",
    "def analyze_model_component(component, name):\n",
    "    params = sum(p.numel() for p in component.parameters())\n",
    "    trainable = sum(p.numel() for p in component.parameters() if p.requires_grad)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Total params: {params:,}\")\n",
    "    print(f\"  Trainable params: {trainable:,}\")\n",
    "    print(f\"  Frozen ratio: {(params-trainable)/params:.1%}\")\n",
    "    return params, trainable\n",
    "\n",
    "vision_params, vision_trainable = analyze_model_component(model.vision_encoder, \"Vision Encoder\")\n",
    "text_params, text_trainable = analyze_model_component(model.text_encoder, \"Text Encoder\")\n",
    "decoder_params, decoder_trainable = analyze_model_component(model.caption_decoder, \"Caption Decoder\")\n",
    "\n",
    "print(\"\\nModel Distribution:\")\n",
    "total = vision_params + text_params + decoder_params\n",
    "print(f\"Vision: {vision_params/total:.1%}\")\n",
    "print(f\"Text: {text_params/total:.1%}\")\n",
    "print(f\"Decoder: {decoder_params/total:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test contrastive mode\n",
    "batch_size = 4\n",
    "images = torch.randn(batch_size, 3, 224, 224)\n",
    "caption_ids = processed_captions['input_ids']\n",
    "caption_mask = processed_captions['attention_mask']\n",
    "\n",
    "with torch.no_grad():\n",
    "    contrastive_outputs = model(\n",
    "        images=images,\n",
    "        caption_ids=caption_ids,\n",
    "        caption_mask=caption_mask,\n",
    "        mode=\"contrastive\"\n",
    "    )\n",
    "\n",
    "print(\"Contrastive Mode Outputs:\")\n",
    "for key, value in contrastive_outputs.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = torch.matmul(\n",
    "    contrastive_outputs['image_embeddings'],\n",
    "    contrastive_outputs['text_embeddings'].t()\n",
    ")\n",
    "\n",
    "print(f\"\\nSimilarity matrix shape: {similarity_matrix.shape}\")\n",
    "print(f\"Similarity range: [{similarity_matrix.min():.3f}, {similarity_matrix.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    similarity_matrix.numpy(),\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='RdYlBu_r',\n",
    "    xticklabels=[f'Caption {i+1}' for i in range(batch_size)],\n",
    "    yticklabels=[f'Image {i+1}' for i in range(batch_size)]\n",
    ")\n",
    "plt.title('Image-Caption Similarity Matrix')\n",
    "plt.xlabel('Captions')\n",
    "plt.ylabel('Images')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Diagonal values (matching pairs)\n",
    "diagonal_similarities = torch.diagonal(similarity_matrix)\n",
    "print(f\"\\nDiagonal similarities (matching pairs): {diagonal_similarities}\")\n",
    "print(f\"Mean diagonal similarity: {diagonal_similarities.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caption Generation Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test caption generation\n",
    "model.eval()\n",
    "\n",
    "# Generate captions for sample images\n",
    "test_images = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_captions = model.generate_captions(\n",
    "        images=test_images,\n",
    "        max_length=30,\n",
    "        num_beams=3,\n",
    "        temperature=0.8,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "print(\"Generated Captions:\")\n",
    "for i, caption in enumerate(generated_captions):\n",
    "    print(f\"  Image {i+1}: '{caption}'\")\n",
    "    print(f\"  Length: {len(caption.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preference_guided_image_captioning_alignment.models.model import ContrastiveLoss, PreferenceLoss\n",
    "\n",
    "# Test contrastive loss\n",
    "contrastive_loss_fn = ContrastiveLoss(temperature=0.07)\n",
    "contrastive_loss = contrastive_loss_fn(\n",
    "    contrastive_outputs['image_embeddings'],\n",
    "    contrastive_outputs['text_embeddings']\n",
    ")\n",
    "\n",
    "print(f\"Contrastive Loss: {contrastive_loss.item():.4f}\")\n",
    "\n",
    "# Test preference loss with dummy data\n",
    "preference_loss_fn = PreferenceLoss(beta=0.1)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "vocab_size = text_processor.vocab_size\n",
    "\n",
    "preferred_logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "rejected_logits = torch.randn(batch_size, seq_len, vocab_size) * 0.8  # Make slightly worse\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "mask = torch.ones(batch_size, seq_len)\n",
    "\n",
    "preference_loss = preference_loss_fn(\n",
    "    preferred_logits, rejected_logits,\n",
    "    labels, labels,\n",
    "    mask, mask\n",
    ")\n",
    "\n",
    "print(f\"Preference Loss: {preference_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Effect Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze effect of temperature on contrastive loss\n",
    "temperatures = np.logspace(-2, 1, 20)  # 0.01 to 10\n",
    "losses = []\n",
    "\n",
    "# Create perfect alignment scenario\n",
    "embeddings = torch.randn(4, 256)\n",
    "embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=-1)\n",
    "\n",
    "for temp in temperatures:\n",
    "    loss_fn = ContrastiveLoss(temperature=temp)\n",
    "    loss = loss_fn(embeddings, embeddings)  # Perfect alignment\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(temperatures, losses, 'b-', linewidth=2, marker='o')\n",
    "plt.axvline(x=0.07, color='r', linestyle='--', label='Default (0.07)')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Contrastive Loss')\n",
    "plt.title('Effect of Temperature on Contrastive Loss (Perfect Alignment)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss at temperature 0.07: {losses[np.argmin(np.abs(temperatures - 0.07))]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preference_guided_image_captioning_alignment.evaluation.metrics import CaptioningMetrics\n",
    "\n",
    "# Create metrics calculator\n",
    "metrics_calc = CaptioningMetrics(device=\"cpu\")\n",
    "\n",
    "# Sample predictions and references\n",
    "predictions = [\n",
    "    \"A red car driving on the street\",\n",
    "    \"A beautiful sunset over the ocean\",\n",
    "    \"A cat sitting on a chair\",\n",
    "    \"Children playing in the park\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    [\"A red sports car on the road\", \"A car driving down the street\"],\n",
    "    [\"Beautiful sunset over water\", \"Golden sunset on the ocean\"],\n",
    "    [\"A cat on a chair\", \"Cat sitting comfortably\"],\n",
    "    [\"Kids playing in park\", \"Children having fun outdoors\"]\n",
    "]\n",
    "\n",
    "print(\"Computing evaluation metrics...\")\n",
    "print(\"Note: Some metrics may take time to load models on first run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU scores\n",
    "bleu_scores = metrics_calc.compute_bleu_scores(predictions, references)\n",
    "print(\"BLEU Scores:\")\n",
    "for metric, score in bleu_scores.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_scores = metrics_calc.compute_rouge_scores(\n",
    "    predictions, [ref[0] for ref in references]\n",
    ")\n",
    "print(\"\\nROUGE Scores:\")\n",
    "for metric, score in rouge_scores.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")\n",
    "\n",
    "# Compute diversity metrics\n",
    "diversity_scores = metrics_calc.compute_diversity_metrics(predictions)\n",
    "print(\"\\nDiversity Metrics:\")\n",
    "for metric, score in diversity_scores.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training configuration\n",
    "stage1_config = config.get_stage1_config()\n",
    "stage2_config = config.get_stage2_config()\n",
    "targets = config.get_targets()\n",
    "\n",
    "print(\"Training Configuration Analysis:\")\n",
    "print(\"\\nStage 1 (Contrastive Learning):\")\n",
    "print(f\"  Batch size: {stage1_config['batch_size']}\")\n",
    "print(f\"  Learning rate: {stage1_config['learning_rate']}\")\n",
    "print(f\"  Epochs: {stage1_config['num_epochs']}\")\n",
    "print(f\"  Warmup steps: {stage1_config['warmup_steps']}\")\n",
    "\n",
    "print(\"\\nStage 2 (Preference Optimization):\")\n",
    "print(f\"  Batch size: {stage2_config['batch_size']}\")\n",
    "print(f\"  Learning rate: {stage2_config['learning_rate']}\")\n",
    "print(f\"  Epochs: {stage2_config['num_epochs']}\")\n",
    "print(f\"  DPO beta: {stage2_config['dpo_beta']}\")\n",
    "\n",
    "print(\"\\nTarget Metrics:\")\n",
    "for target, value in targets.items():\n",
    "    print(f\"  {target}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training schedule\n",
    "def simulate_lr_schedule(initial_lr, warmup_steps, total_steps):\n",
    "    \"\"\"Simulate cosine learning rate schedule with warmup.\"\"\"\n",
    "    steps = np.arange(total_steps)\n",
    "    lrs = []\n",
    "    \n",
    "    for step in steps:\n",
    "        if step < warmup_steps:\n",
    "            # Linear warmup\n",
    "            lr = initial_lr * (step / warmup_steps)\n",
    "        else:\n",
    "            # Cosine decay\n",
    "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            lr = initial_lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "        lrs.append(lr)\n",
    "    \n",
    "    return steps, lrs\n",
    "\n",
    "# Simulate learning rate schedules for both stages\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Stage 1 schedule\n",
    "steps1, lrs1 = simulate_lr_schedule(\n",
    "    stage1_config['learning_rate'],\n",
    "    stage1_config['warmup_steps'],\n",
    "    stage1_config['num_epochs'] * 100  # Assume 100 steps per epoch\n",
    ")\n",
    "\n",
    "ax1.plot(steps1, lrs1, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Training Steps')\n",
    "ax1.set_ylabel('Learning Rate')\n",
    "ax1.set_title('Stage 1: Contrastive Learning Schedule')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Stage 2 schedule\n",
    "steps2, lrs2 = simulate_lr_schedule(\n",
    "    stage2_config['learning_rate'],\n",
    "    stage2_config['warmup_steps'],\n",
    "    stage2_config['num_epochs'] * 50  # Assume 50 steps per epoch\n",
    ")\n",
    "\n",
    "ax2.plot(steps2, lrs2, 'r-', linewidth=2)\n",
    "ax2.set_xlabel('Training Steps')\n",
    "ax2.set_ylabel('Learning Rate')\n",
    "ax2.set_title('Stage 2: Preference Optimization Schedule')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Stage 1 LR range: {min(lrs1):.2e} - {max(lrs1):.2e}\")\n",
    "print(f\"Stage 2 LR range: {min(lrs2):.2e} - {max(lrs2):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze memory usage and computational complexity\n",
    "import time\n",
    "\n",
    "def measure_forward_pass_time(model, batch_size, num_runs=10):\n",
    "    \"\"\"Measure forward pass time for different batch sizes.\"\"\"\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(3):\n",
    "            images = torch.randn(batch_size, 3, 224, 224)\n",
    "            caption_ids = torch.randint(0, 1000, (batch_size, 32))\n",
    "            caption_mask = torch.ones(batch_size, 32)\n",
    "            _ = model(images, caption_ids, caption_mask, mode=\"contrastive\")\n",
    "    \n",
    "    # Actual measurement\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            images = torch.randn(batch_size, 3, 224, 224)\n",
    "            caption_ids = torch.randint(0, 1000, (batch_size, 32))\n",
    "            caption_mask = torch.ones(batch_size, 32)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            _ = model(images, caption_ids, caption_mask, mode=\"contrastive\")\n",
    "            end_time = time.time()\n",
    "            \n",
    "            times.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "# Test different batch sizes\n",
    "batch_sizes = [1, 2, 4, 8, 16]\n",
    "mean_times = []\n",
    "std_times = []\n",
    "\n",
    "print(\"Measuring inference times for different batch sizes...\")\n",
    "for batch_size in batch_sizes:\n",
    "    mean_time, std_time = measure_forward_pass_time(model, batch_size, num_runs=5)\n",
    "    mean_times.append(mean_time)\n",
    "    std_times.append(std_time)\n",
    "    print(f\"Batch size {batch_size:2d}: {mean_time:6.2f} Â± {std_time:5.2f} ms\")\n",
    "\n",
    "# Calculate per-sample times\n",
    "per_sample_times = [mean_time / batch_size for mean_time, batch_size in zip(mean_times, batch_sizes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Total time vs batch size\n",
    "ax1.errorbar(batch_sizes, mean_times, yerr=std_times, \n",
    "             marker='o', linewidth=2, capsize=5)\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Forward Pass Time (ms)')\n",
    "ax1.set_title('Total Forward Pass Time vs Batch Size')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Per-sample time vs batch size\n",
    "ax2.plot(batch_sizes, per_sample_times, 'ro-', linewidth=2)\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Per-Sample Time (ms)')\n",
    "ax2.set_title('Per-Sample Forward Pass Time vs Batch Size')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate efficiency metrics\n",
    "baseline_time = per_sample_times[0]  # Single sample time\n",
    "efficiency_gains = [baseline_time / per_sample_time for per_sample_time in per_sample_times]\n",
    "\n",
    "print(\"\\nBatch Processing Efficiency:\")\n",
    "for batch_size, efficiency in zip(batch_sizes, efficiency_gains):\n",
    "    print(f\"Batch size {batch_size:2d}: {efficiency:.2f}x speedup\")\n",
    "\n",
    "target_latency = targets.get('latency_ms_p95', 150)\n",
    "max_efficient_batch = max([bs for bs, time in zip(batch_sizes, per_sample_times) \n",
    "                          if time <= target_latency])\n",
    "print(f\"\\nMaximum efficient batch size (â‰¤{target_latency}ms per sample): {max_efficient_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PREFERENCE-GUIDED IMAGE CAPTIONING - EXPLORATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸ—ï¸  MODEL ARCHITECTURE:\")\n",
    "print(f\"   â€¢ Total parameters: {total_params:,}\")\n",
    "print(f\"   â€¢ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   â€¢ Projection dimension: {model.projection_dim}\")\n",
    "print(f\"   â€¢ Components: Vision Encoder + Text Encoder + Caption Decoder\")\n",
    "\n",
    "print(\"\\nðŸ”„  TRAINING STRATEGY:\")\n",
    "print(f\"   â€¢ Stage 1: Contrastive learning (LR: {stage1_config['learning_rate']})\")\n",
    "print(f\"   â€¢ Stage 2: Preference optimization (LR: {stage2_config['learning_rate']})\")\n",
    "print(f\"   â€¢ DPO beta: {stage2_config['dpo_beta']}\")\n",
    "print(f\"   â€¢ Temperature: {model.temperature}\")\n",
    "\n",
    "print(\"\\nðŸ“Š  TARGET METRICS:\")\n",
    "for target, value in targets.items():\n",
    "    print(f\"   â€¢ {target}: {value}\")\n",
    "\n",
    "print(\"\\nâš¡  PERFORMANCE INSIGHTS:\")\n",
    "print(f\"   â€¢ Optimal batch size for latency: {max_efficient_batch}\")\n",
    "print(f\"   â€¢ Max batching speedup: {max(efficiency_gains):.2f}x\")\n",
    "print(f\"   â€¢ Single sample latency: ~{per_sample_times[0]:.1f}ms\")\n",
    "\n",
    "print(\"\\nðŸŽ¯  KEY INNOVATIONS:\")\n",
    "print(\"   â€¢ Dual-stage training: contrastive + preference alignment\")\n",
    "print(\"   â€¢ Cross-modal attention in caption decoder\")\n",
    "print(\"   â€¢ LoRA support for efficient fine-tuning\")\n",
    "print(\"   â€¢ Comprehensive evaluation with 10+ metrics\")\n",
    "print(\"   â€¢ Human preference correlation tracking\")\n",
    "\n",
    "print(\"\\nðŸ”  TECHNICAL HIGHLIGHTS:\")\n",
    "print(\"   â€¢ CLIP-based vision encoder for robust image understanding\")\n",
    "print(\"   â€¢ GPT-based text encoder and decoder for natural language\")\n",
    "print(\"   â€¢ DPO-style preference optimization\")\n",
    "print(\"   â€¢ Configurable temperature for contrastive learning\")\n",
    "print(\"   â€¢ Mixed precision training support\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… EXPLORATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\nThis exploration demonstrates the key capabilities of our preference-guided image captioning system:\n\n1. **Model Architecture**: Multi-component design with vision encoder, text encoder, and caption decoder\n2. **Training Pipeline**: Two-stage approach combining contrastive learning and preference optimization\n3. **Evaluation Framework**: Comprehensive metrics including BLEU, ROUGE, CIDEr, and preference alignment\n4. **Performance Analysis**: Efficient batch processing with configurable latency targets\n\n### Recommended Training Workflow:\n\n```bash\n# 1. Train the full pipeline\npython scripts/train.py --config configs/default.yaml\n\n# 2. Evaluate the trained model\npython scripts/run_evaluation.py --checkpoint outputs/checkpoints/best_model_stage2.pt\n\n# 3. Run specific stages if needed\npython scripts/train.py --stage 1  # Contrastive learning only\npython scripts/train.py --stage 2  # Preference optimization only\n```\n\n### Configuration Tips:\n\n- Adjust `projection_dim` for different performance/quality trade-offs\n- Tune `temperature` in contrastive loss for better alignment\n- Modify `dpo_beta` to control preference optimization strength\n- Use LoRA configuration for memory-efficient fine-tuning\n\n### Expected Results:\n\nWith proper training, the model should achieve:\n- CIDEr score >= 1.15\n- Preference win rate >= 72%\n- Human evaluation helpfulness >= 4.2/5\n- P95 latency <= 150ms"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}